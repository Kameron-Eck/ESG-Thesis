{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d211a168-041c-464f-a216-9e6f11cbdfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "214c2802-491e-42d7-b7e9-bc9eb50d26f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/ISS_B_M.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Normalize gender representation by converting 'Yes' and 'No' to 'Female' and 'Male'\n",
    "df['gender'] = df['female'].map({'Yes': 'Female', 'No': 'Male'})\n",
    "\n",
    "# Calculate average age per year and per company\n",
    "average_age = df.groupby(['year', 'company_id'])['age'].mean()\n",
    "\n",
    "# Pivot tables for ethnicity, classification, and gender fractions\n",
    "ethnicity_pivot = pd.pivot_table(df, index=['year', 'company_id'], columns='ethnicity', aggfunc='size', fill_value=0)\n",
    "classification_pivot = pd.pivot_table(df, index=['year', 'company_id'], columns='Classification', aggfunc='size', fill_value=0)\n",
    "gender_pivot = pd.pivot_table(df, index=['year', 'company_id'], columns='gender', aggfunc='size', fill_value=0)\n",
    "\n",
    "# Normalize the rows to get fractions\n",
    "ethnicity_fractions = ethnicity_pivot.div(ethnicity_pivot.sum(axis=1), axis=0)\n",
    "classification_fractions = classification_pivot.div(classification_pivot.sum(axis=1), axis=0)\n",
    "gender_fractions = gender_pivot.div(gender_pivot.sum(axis=1), axis=0)\n",
    "\n",
    "# Combine average age with the ethnicity, classification, and gender fractions\n",
    "final_df = pd.concat([average_age, ethnicity_fractions, classification_fractions, gender_fractions], axis=1).reset_index()\n",
    "\n",
    "# Output to CSV, preserving the 'year' and 'company_id' columns\n",
    "final_df.to_csv('/Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/final_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa5ad3c3-55dd-4b15-a0df-b481af743bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '/Users/Eartheconomics/Downloads/dime_contributors_1979_2022.csv'\n",
    "\n",
    "# Load the DataFrame with a specified encoding\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1', chunksize=10000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79d12c42-da64-4891-85ee-2224114057be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bonica.cid', 'contributor.type', 'num.distinct', 'most.recent.contributor.name', 'most.recent.contributor.address', 'most.recent.contributor.city', 'most.recent.contributor.zipcode', 'most.recent.contributor.state', 'most.recent.contributor.latitude', 'most.recent.contributor.longitude', 'most.recent.contributor.occupation', 'most.recent.contributor.employer', 'most.recent.transaction.id', 'most.recent.transaction.date', 'contributor.gender', 'is.corp', 'contributor.cfscore', 'is.projected', 'first_cycle_active', 'last_cycle_active', 'amount.1980', 'amount.1982', 'amount.1984', 'amount.1986', 'amount.1988', 'amount.1990', 'amount.1992', 'amount.1994', 'amount.1996', 'amount.1998', 'amount.2000', 'amount.2002', 'amount.2004', 'amount.2006', 'amount.2008', 'amount.2010', 'amount.2012', 'amount.2014', 'amount.2016', 'amount.2018', 'amount.2020', 'amount.2022']\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file. Here, we use nrows=0 to avoid loading data, just the headers.\n",
    "head = pd.read_csv(file_path, nrows=0)\n",
    "\n",
    "# Get the list of column headers\n",
    "column_headers = head.columns.tolist()\n",
    "\n",
    "# Print the column headers\n",
    "print(column_headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c91e5f9-a4bf-498b-a7a3-81d3a5a6167c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m     sorted_chunk \u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis.corp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmost.recent.contributor.name\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Append the sorted chunk to the main DataFrame\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     filtered_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([filtered_df, sorted_chunk], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Sorting again in case of overlaps between chunks\u001b[39;00m\n\u001b[1;32m     27\u001b[0m filtered_df\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis.corp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmost.recent.contributor.name\u001b[39m\u001b[38;5;124m'\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Volumes/S/opt/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/concat.py:393\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    378\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    380\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    381\u001b[0m     objs,\n\u001b[1;32m    382\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    391\u001b[0m )\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/Volumes/S/opt/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/concat.py:680\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[1;32m    678\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[0;32m--> 680\u001b[0m new_data \u001b[38;5;241m=\u001b[39m concatenate_managers(\n\u001b[1;32m    681\u001b[0m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_axes, concat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm_axis, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m    682\u001b[0m )\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    684\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[0;32m/Volumes/S/opt/anaconda3/lib/python3.11/site-packages/pandas/core/internals/concat.py:177\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m    167\u001b[0m vals \u001b[38;5;241m=\u001b[39m [ju\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units]\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_extension:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# expected \"Union[_SupportsArray[dtype[Any]],\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# _NestedSequence[_SupportsArray[dtype[Any]]]]\"\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_1d_only_ea_dtype(blk\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     values \u001b[38;5;241m=\u001b[39m concat_compat(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ea_compat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "file_path = '/Users/Eartheconomics/Downloads/dime_contributors_1979_2022.csv'\n",
    "\n",
    "# Columns you want to keep\n",
    "columns = [\n",
    "    'bonica.cid', 'contributor.type', 'num.distinct', 'most.recent.contributor.name', \n",
    "    'most.recent.contributor.occupation', 'most.recent.contributor.employer', 'contributor.gender', \n",
    "    'is.corp',  'amount.1980', 'amount.1982', 'amount.1984', 'amount.1986', 'amount.1988', \n",
    "    'amount.1990', 'amount.1992', 'amount.1994', 'amount.1996', 'amount.1998', 'amount.2000', \n",
    "    'amount.2002', 'amount.2004', 'amount.2006', 'amount.2008', 'amount.2010', 'amount.2012', \n",
    "    'amount.2014', 'amount.2016', 'amount.2018', 'amount.2020', 'amount.2022'\n",
    "]\n",
    "\n",
    "# Initialize an empty DataFrame to hold the results\n",
    "filtered_df = pd.DataFrame()\n",
    "\n",
    "# Load the data in chunks\n",
    "for chunk in pd.read_csv(file_path, chunksize=10000, encoding='ISO-8859-1', usecols=columns):\n",
    "    # Sort by 'is.corp' and 'most.recent.contributor.name' within each chunk\n",
    "    sorted_chunk = chunk.sort_values(by=['is.corp', 'most.recent.contributor.name'])\n",
    "    # Append the sorted chunk to the main DataFrame\n",
    "    filtered_df = pd.concat([filtered_df, sorted_chunk], ignore_index=True)\n",
    "\n",
    "# Sorting again in case of overlaps between chunks\n",
    "filtered_df.sort_values(by=['is.corp', 'most.recent.contributor.name'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "233dc741-7a6e-4584-b37a-a156b183833f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'TextFileReader' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000000\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Number of rows per Excel file\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m rows_per_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m44\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m44\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     11\u001b[0m     rows_per_file \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'TextFileReader' has no len()"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the CSV file with low_memory set to False and specifying encoding\n",
    "file_path = '/Users/Eartheconomics/Downloads/dime_contributors_1979_2022.csv'\n",
    "df = pd.read_csv(file_path, chunksize=1000000, encoding='ISO-8859-1')\n",
    "\n",
    "# Number of rows per Excel file\n",
    "rows_per_file = len(df) // 44\n",
    "if len(df) % 44 != 0:\n",
    "    rows_per_file += 1\n",
    "\n",
    "# Directory to save the Excel files\n",
    "output_dir = '/Users/Eartheconomics/Downloads/Split_Excel_Files'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Split and save the data into multiple Excel files\n",
    "for i in range(44):\n",
    "    start_row = i * rows_per_file\n",
    "    end_row = start_row + rows_per_file\n",
    "    df_subset = df.iloc[start_row:min(end_row, len(df))]\n",
    "    output_filename = os.path.join(output_dir, f'part_{i+1}.xlsx')\n",
    "    df_subset.to_excel(output_filename, index=False)\n",
    "    print(f'Saved {output_filename}')\n",
    "\n",
    "print(\"All files have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a6c8d8-47ae-4376-9379-a6318903e611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 41603104 rows.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17837b4d-af50-4fdc-b649-2b98fd30ba23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0043c008-b35f-4b37-84f7-7689236d48c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y9/_z2g89050z1_2z8f5pql6klr0000gn/T/ipykernel_17059/997717786.py:7: DtypeWarning: Columns (4,5,10,11,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding='ISO-8859-1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/1.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/2.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/3.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/4.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/5.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/6.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/7.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/8.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/9.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/10.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/11.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/12.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/13.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/14.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/15.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/16.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/17.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/18.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/19.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/20.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/21.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/22.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/23.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/24.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/25.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/26.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/27.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/28.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/29.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/30.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/31.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/32.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/33.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/34.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/35.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/36.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/37.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/38.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/39.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/40.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/41.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/42.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/43.csv\n",
      "Saved /Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors/44.csv\n",
      "Data has been split into 44 files.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "file_path = '/Users/Eartheconomics/Downloads/dime_contributors_1979_2022.csv'\n",
    "\n",
    "# Load the entire DataFrame\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "# Determine the number of rows in each split\n",
    "num_rows = df.shape[0]\n",
    "rows_per_file = num_rows // 44\n",
    "\n",
    "# Directory to save the CSV files\n",
    "output_dir = '/Users/Eartheconomics/Desktop/S_J/ESG Thesis/Data/Board of Directors'\n",
    "\n",
    "# Split the DataFrame and save each part\n",
    "for i in range(44):\n",
    "    start_index = i * rows_per_file\n",
    "    if i == 43:  # Ensure the last file includes any remaining rows due to integer division\n",
    "        end_index = num_rows\n",
    "    else:\n",
    "        end_index = start_index + rows_per_file\n",
    "    \n",
    "    # Slice the DataFrame\n",
    "    df_slice = df.iloc[start_index:end_index]\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_filename = f'{output_dir}/{i + 1}.csv'\n",
    "    df_slice.to_csv(output_filename, index=False)\n",
    "    print(f'Saved {output_filename}')\n",
    "\n",
    "print(\"Data has been split into 44 files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59835026-6b32-4da4-afda-5368bfcce9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
