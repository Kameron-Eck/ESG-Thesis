{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Before running this code, I did the manual work of sorting Open Secrets Campaign Finance data into separate folders. The folders were titled based on the content type (e.g. Individual Contribution, Pac Contributions, etc.).",
   "id": "a745d741f22768d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "List of all Congressional Canidates",
   "id": "f2d04475d8f5185c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T18:49:12.380640Z",
     "start_time": "2024-08-16T18:49:11.357409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import concurrent.futures\n",
    "\n",
    "# Path to the CSV files\n",
    "cands = r'C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\cands\\*.txt'\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "all_files = glob.glob(cands)\n",
    "\n",
    "# List of custom headers\n",
    "headers = [\"Field\", \"Cycle\", \"FECCandID\", \"CID\", \"FirstLastP\", \"Party\", \"DistIDRunFor\", \"DistIDCurr\", \"CurrCand\", \"CycleCand\", \"CRPICO\", \"RecipCode\", \"NoPacs\"]\n",
    "\n",
    "# Function to read a single CSV file with specified headers\n",
    "def read_csv(file):\n",
    "    try:\n",
    "        df = pd.read_csv(file, encoding='ISO-8859-1', delimiter=',', quotechar='|', names=headers, low_memory=False)\n",
    "        print(f\"Successfully read {file}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use ThreadPoolExecutor to read files in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    dfs = list(executor.map(read_csv, all_files))\n",
    "\n",
    "# Concatenate all the dataframes in the list into a single dataframe\n",
    "if dfs:\n",
    "    big_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Shape of the concatenated dataframe: {big_df.shape}\")\n",
    "    \n",
    "    # Save the concatenated dataframe to a single CSV file\n",
    "    output_path = r'C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\cands.csv'\n",
    "    big_df.to_csv(output_path, index=False)\n",
    "    print(f\"Concatenated file saved to {output_path}\")\n",
    "else:\n",
    "    print(\"No dataframes to concatenate.\")\n"
   ],
   "id": "e69b6989715b714a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\cands\\cands14.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\cands\\cands20.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\cands\\cands12.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\cands\\cands10.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\cands\\cands22.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\cands\\cands18.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\cands\\cands16.txt\n",
      "Shape of the concatenated dataframe: (49077, 13)\n",
      "Concatenated file saved to C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\cands.csv\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "List of Committees",
   "id": "11b0e57fc7fd20fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T18:49:14.605379Z",
     "start_time": "2024-08-16T18:49:13.508138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import concurrent.futures\n",
    "\n",
    "# Path to the CSV files\n",
    "cmtes = r'C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\cmtes\\*.txt'\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "all_files = glob.glob(cmtes)\n",
    "\n",
    "# List of custom headers\n",
    "headers = [\"Cycle\", \"CmteID\", \"PACShort\", \"Affiliate\", \"Ultorg\", \"RecipID\", \"RecipCode\", \"FECCandID\", \"Party\", \"PrimCode\", \"Source\", \"Sensitive\", \"Foreign\", \"Active\"]\n",
    "\n",
    "# Function to read a single CSV file with specified headers\n",
    "def read_csv(file):\n",
    "    try:\n",
    "        df = pd.read_csv(file, encoding='ISO-8859-1', delimiter=',', quotechar='|', names=headers, low_memory=False)\n",
    "        print(f\"Successfully read {file}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use ThreadPoolExecutor to read files in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    dfs = list(executor.map(read_csv, all_files))\n",
    "\n",
    "# Filter out any None values in the list\n",
    "dfs = [df for df in dfs if df is not None]\n",
    "\n",
    "# Concatenate all the dataframes in the list into a single dataframe\n",
    "if dfs:\n",
    "    big_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Shape of the concatenated dataframe: {big_df.shape}\")\n",
    "    \n",
    "    # Save the concatenated dataframe to a single CSV file\n",
    "    output_path = r'C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\cmtes.csv'\n",
    "    big_df.to_csv(output_path, index=False)\n",
    "    print(f\"Concatenated file saved to {output_path}\")\n",
    "else:\n",
    "    print(\"No dataframes to concatenate.\")\n"
   ],
   "id": "313a6dc8b1374d9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\cmtes\\cmtes10.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\cmtes\\cmtes12.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\cmtes\\cmtes16.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\cmtes\\cmtes14.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\cmtes\\cmtes22.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\cmtes\\cmtes20.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\cmtes\\cmtes18.txt\n",
      "Shape of the concatenated dataframe: (115651, 14)\n",
      "Concatenated file saved to C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\cmtes.csv\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T18:49:29.989713Z",
     "start_time": "2024-08-16T18:49:14.606896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import concurrent.futures\n",
    "\n",
    "# Path to the CSV files\n",
    "pacs2cand = r'C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\pacs\\*.txt'\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "all_files = glob.glob(pacs2cand)\n",
    "\n",
    "# List of custom headers\n",
    "headers = [\"Cycle\", \"FECRecNo\", \"PACID\", \"CID\", \"Amount\", \"Date\", \"RealCode\", \"Type\", \"DI\", \"FECCandID\"]\n",
    "\n",
    "# Function to read a single CSV file with specified headers\n",
    "def read_csv(file):\n",
    "    try:\n",
    "        df = pd.read_csv(file, encoding='ISO-8859-1', delimiter=',', quotechar='|', names=headers, low_memory=False)\n",
    "        print(f\"Successfully read {file}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use ThreadPoolExecutor to read files in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    dfs = list(executor.map(read_csv, all_files))\n",
    "\n",
    "# Filter out any None values in the list\n",
    "dfs = [df for df in dfs if df is not None]\n",
    "\n",
    "# Concatenate all the dataframes in the list into a single dataframe\n",
    "if dfs:\n",
    "    big_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Shape of the concatenated dataframe: {big_df.shape}\")\n",
    "    \n",
    "    # Save the concatenated dataframe to a single CSV file\n",
    "    output_path = r'C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\pacs.csv'\n",
    "    big_df.to_csv(output_path, index=False)\n",
    "    print(f\"Concatenated file saved to {output_path}\")\n",
    "else:\n",
    "    print(\"No dataframes to concatenate.\")\n"
   ],
   "id": "c143560c2b289ba4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\pacs\\pacs12.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\pacs\\pacs10.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\pacs\\pacs14.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\pacs\\pacs18.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\pacs\\pacs16.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\pacs\\pacs22.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\pacs\\pacs20.txt\n",
      "Shape of the concatenated dataframe: (3592977, 10)\n",
      "Concatenated file saved to C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\pacs.csv\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "List of Pac contributions to canidates",
   "id": "b0579cccc483a8ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-16T18:49:59.153373Z",
     "start_time": "2024-08-16T18:49:29.989713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import concurrent.futures\n",
    "\n",
    "# Path to the CSV files\n",
    "pac2pac = r'C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\pac_other\\*.txt'\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "all_files = glob.glob(pac2pac)\n",
    "\n",
    "# List of custom headers\n",
    "headers = [\n",
    "    \"Cycle\", \"FECRecNo\", \"Filerid\", \"DonorCmte\", \"ContribLendTrans\", \"City\", \"State\", \n",
    "    \"Zip\", \"FECOccEmp\", \"Primcode\", \"Date\", \"Amount\", \"RecipID\", \"Party\", \"Otherid\", \n",
    "    \"RecipCode\", \"RecipPrimcode\", \"Amend\", \"Report\", \"PG\", \"Microfilm\", \"Type\", \"RealCode\", \"Source\"\n",
    "]\n",
    "\n",
    "# Function to read a single CSV file with specified headers\n",
    "def read_csv(file):\n",
    "    try:\n",
    "        df = pd.read_csv(file, encoding='ISO-8859-1', delimiter=',', quotechar='|', names=headers, low_memory=False)\n",
    "        print(f\"Successfully read {file}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use ThreadPoolExecutor to read files in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    dfs = list(executor.map(read_csv, all_files))\n",
    "\n",
    "# Filter out any None values in the list\n",
    "dfs = [df for df in dfs if df is not None]\n",
    "\n",
    "# Concatenate all the dataframes in the list into a single dataframe\n",
    "if dfs:\n",
    "    big_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Shape of the concatenated dataframe: {big_df.shape}\")\n",
    "    \n",
    "    # Save the concatenated dataframe to a single CSV file\n",
    "    output_path = r'C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\pac2pac.csv'\n",
    "    big_df.to_csv(output_path, index=False)\n",
    "    print(f\"Concatenated file saved to {output_path}\")\n",
    "else:\n",
    "    print(\"No dataframes to concatenate.\")\n"
   ],
   "id": "4ab0731b9c8e3ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\pac_other\\pac_other10.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\pac_other\\pac_other14.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\pac_other\\pac_other12.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\pac_other\\pac_other16.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\pac_other\\pac_other18.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\pac_other\\pac_other20.txt\n",
      "Successfully read C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\pac_other\\pac_other22.txt\n",
      "Shape of the concatenated dataframe: (2631064, 24)\n",
      "Concatenated file saved to C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\pac2pac.csv\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "List of Citizens donating to poltical canidates",
   "id": "a343e1cf2267e3f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T00:14:25.452582Z",
     "start_time": "2024-08-17T00:13:56.554470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import concurrent.futures\n",
    "\n",
    "# Path to the CSV files\n",
    "indiv = r'C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\Campaign Finance\\indivs\\*.txt'\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "all_files = glob.glob(indiv)\n",
    "\n",
    "# List of custom headers and their associated data types\n",
    "data_types = {\n",
    "    \"Cycle\": str,\n",
    "    \"FECTransID\": str,\n",
    "    \"ContribID\": str,\n",
    "    \"Contrib\": str,\n",
    "    \"RecipID\": str,\n",
    "    \"Orgname\": str,\n",
    "    \"UltOrg\": str,\n",
    "    \"RealCode\": str,\n",
    "    \"Date\": str,  # Consider converting this to datetime after reading\n",
    "    \"Amount\": 'int64',\n",
    "    \"Street\": str,\n",
    "    \"City\": str,\n",
    "    \"State\": str,\n",
    "    \"Zip\": str,\n",
    "    \"RecipCode\": str,\n",
    "    \"Type\": str,\n",
    "    \"CmteID\": str,\n",
    "    \"OtherID\": str,\n",
    "    \"Gender\": str,\n",
    "    \"Microfilm\": str,\n",
    "    \"Occupation\": str,\n",
    "    \"Employer\": str,\n",
    "    \"Source\": str\n",
    "}\n",
    "\n",
    "# Function to read a single CSV file with specified headers and data types\n",
    "def read_csv(file):\n",
    "    try:\n",
    "        df = pd.read_csv(file, encoding='ISO-8859-1', delimiter=',', quotechar='|', dtype=data_types, names=list(data_types.keys()), low_memory=False)\n",
    "        print(f\"Successfully read {file}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Use ThreadPoolExecutor to read files in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    dfs = list(executor.map(read_csv, all_files))\n",
    "\n",
    "# Filter out any None values in the list\n",
    "dfs = [df for df in dfs if df is not None]\n",
    "\n",
    "# Concatenate all the dataframes in the list into a single dataframe\n",
    "if dfs:\n",
    "    big_df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"Shape of the concatenated dataframe: {big_df.shape}\")\n",
    "    \n",
    "    # Save the concatenated dataframe to a single CSV file\n",
    "    output_path = r'C:\\Users\\Kameron\\Documents\\ESG Thesis\\Data\\Political_Contributions\\indivss.csv'\n",
    "    big_df.to_csv(output_path, index=False)\n",
    "    print(f\"Concatenated file saved to {output_path}\")\n",
    "else:\n",
    "    print(\"No dataframes to concatenate.\")\n"
   ],
   "id": "34f33748a58571d2",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Table schema does not match schema used to create file: \ntable:\nCycle: string\nFECTransID: string\nContribID: string\nContrib: string\nRecipID: string\nOrgname: string\nUltOrg: string\nRealCode: string\nDate: string\nAmount: int64\nStreet: null\nCity: string\nState: string\nZip: string\nRecipCode: string\nType: string\nCmteID: string\nOtherID: null\nGender: string\nMicrofilm: string\nOccupation: string\nEmployer: string\nSource: string\n-- schema metadata --\npandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 6300' + 2866 vs. \nfile:\nCycle: string\nFECTransID: string\nContribID: string\nContrib: string\nRecipID: string\nOrgname: string\nUltOrg: string\nRealCode: string\nDate: string\nAmount: int64\nStreet: null\nCity: string\nState: string\nZip: string\nRecipCode: string\nType: string\nCmteID: string\nOtherID: string\nGender: string\nMicrofilm: string\nOccupation: string\nEmployer: string\nSource: string\n-- schema metadata --\npandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 2862",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 60\u001B[0m\n\u001B[0;32m     58\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m parquet_writer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     59\u001B[0m             parquet_writer \u001B[38;5;241m=\u001B[39m pq\u001B[38;5;241m.\u001B[39mParquetWriter(output_path, schema, compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msnappy\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 60\u001B[0m         \u001B[43mparquet_writer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_table\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtable\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m parquet_writer:\n\u001B[0;32m     63\u001B[0m     parquet_writer\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\PycharmProjects\\ESG\\pythonProject\\.venv\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1114\u001B[0m, in \u001B[0;36mParquetWriter.write_table\u001B[1;34m(self, table, row_group_size)\u001B[0m\n\u001B[0;32m   1110\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m table\u001B[38;5;241m.\u001B[39mschema\u001B[38;5;241m.\u001B[39mequals(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mschema, check_metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m   1111\u001B[0m     msg \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTable schema does not match schema used to create file: \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   1112\u001B[0m            \u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mtable:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{!s}\u001B[39;00m\u001B[38;5;124m vs. \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mfile:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{!s}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   1113\u001B[0m            \u001B[38;5;241m.\u001B[39mformat(table\u001B[38;5;241m.\u001B[39mschema, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mschema))\n\u001B[1;32m-> 1114\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[0;32m   1116\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwriter\u001B[38;5;241m.\u001B[39mwrite_table(table, row_group_size\u001B[38;5;241m=\u001B[39mrow_group_size)\n",
      "\u001B[1;31mValueError\u001B[0m: Table schema does not match schema used to create file: \ntable:\nCycle: string\nFECTransID: string\nContribID: string\nContrib: string\nRecipID: string\nOrgname: string\nUltOrg: string\nRealCode: string\nDate: string\nAmount: int64\nStreet: null\nCity: string\nState: string\nZip: string\nRecipCode: string\nType: string\nCmteID: string\nOtherID: null\nGender: string\nMicrofilm: string\nOccupation: string\nEmployer: string\nSource: string\n-- schema metadata --\npandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 6300' + 2866 vs. \nfile:\nCycle: string\nFECTransID: string\nContribID: string\nContrib: string\nRecipID: string\nOrgname: string\nUltOrg: string\nRealCode: string\nDate: string\nAmount: int64\nStreet: null\nCity: string\nState: string\nZip: string\nRecipCode: string\nType: string\nCmteID: string\nOtherID: string\nGender: string\nMicrofilm: string\nOccupation: string\nEmployer: string\nSource: string\n-- schema metadata --\npandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 2862"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a6a660ef27c29d6c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
